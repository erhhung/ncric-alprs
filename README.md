# ALPRS Infrastructure

## Local Environment

Terraform and other deployment tools have been baked into a custom Docker image that will be built and run by "`tf.sh`".  
So, aside from **Docker** itself, very few tools need to be installed in the local environment.

Of course, AWS credentials under "`~/.aws`" and SSH keys under "`~/.ssh`" are still required, as is a properly formatted  
"`~/.ssh/config`" (see "[`ssh-config`](ssh-config)" for example) to get updated by "`./upssh.sh`" after "`./tf.sh apply`".

The following tools should be installed _(assumes macOS environment using Homebrew)_:

* **`docker`** â€”Â `brew install --cask docker`
* **`aws`**    â€”Â `brew install awscli`
* **`jq`**     â€”Â `brew install jq`
* **`pee`**    â€”Â `brew install moreutils`
* **`delta`**  â€”Â `brew install git-delta`

## AWS Environment

### AWS Systems Manager

* **Quick Setup**
  * Enable all **Host Management** configuration options

## Terraform State

Create S3 bucket for Terraform to store its state _(assumes
**AWS CLI profiles `alprscom` or `alprsgov` already exist!**)_:

```bash
# adjust settings accordingly for the prod environment
AWS_PROFILE=alprscom
BUCKET=alprs-infra-dev
REGION=us-west-2

aws s3api create-bucket \
  --bucket $BUCKET \
  --create-bucket-configuration "LocationConstraint=$REGION" \
  --object-ownership BucketOwnerEnforced

aws s3api put-public-access-block \
  --bucket $BUCKET \
  --public-access-block-configuration 'BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true'

aws s3api put-bucket-encryption \
  --bucket $BUCKET \
  --server-side-encryption-configuration '{"Rules":[{"ApplyServerSideEncryptionByDefault":{"SSEAlgorithm":"AES256"}}]}'
```

### `conf.sh` â€” Config Tool

Environment-specific configuration file "`config/dev.tfvars`" or "`config/prod.tfvars`"
(there should **only be one** of these in "`config/`")  
can be pulled from, pushed to, and compared with their remote copy in S3 (in the same
folder as the Terraform state file) using the `conf.sh` script:

```bash
$ ./conf.sh diff
Retrieving Terraform output variables...DONE.
File not found: config/dev.tfvars

$ ./conf.sh pull
Retrieving Terraform output variables...DONE.
Writing: config/dev.tfvars
download: s3://alprs-infra-dev/tfstate/dev.tfvars to ./dev.tfvars

$ ./conf.sh diff
Reading: s3://alprs-infra-dev/tfstate/dev.tfvars
download: s3://alprs-infra-dev/tfstate/dev.tfvars to ./dev.tfvars.remote
ğŸ‘ No differences found.

$ ./conf.sh push
Writing: s3://alprs-infra-dev/tfstate/dev.tfvars.backup
copy: s3://alprs-infra-dev/tfstate/dev.tfvars to s3://alprs-infra-dev/tfstate/dev.tfvars.backup
Writing: s3://alprs-infra-dev/tfstate/dev.tfvars
upload: ./dev.tfvars to s3://alprs-infra-dev/tfstate/dev.tfvars
```

## Terraform Init

```bash
./tf.sh init -backend-config config/dev.conf
```

## Terraform Apply

_Due to some `for_each` values that depend on resource attributes which cannot be determined  
until after apply, the `-target` option must be used to create those dependent resources first._

```bash
# check whether your "config/dev.tfvars" is up-to-date,
# and download it from the infra S3 bucket if you don't
# have one (it contains secrets and is not checked into
# GitHub) or is out-of-date
./conf.sh diff # and "pull"

# explicit -target is only necessary if provisioning the tfstate for the first time
./tf.sh apply -var-file config/dev.tfvars -target aws_ses_domain_dkim.astrometrics

./tf.sh apply -var-file config/dev.tfvars
```

## EC2 Instances

Seven EC2 instances are provisioned by Terraform:

* **PostgreSQL**Â â€”Â Primary "alprs" (a.k.a. "Black Panther") database; staging (a.k.a. "Atlas") database for NCRIC org; Rundeck datastore
* **Elasticsearch**Â â€” Exposes ports for both HTTP and transport; Kibana on HTTPS
* **Conductor** â€”Â Catalogs ingested ALPR reads in database; triggers indexing
* **Datastore**Â â€”Â REST API server; manages storage of ingested ALPR images
* **Indexer**Â â€” Sends ingested ALPR reads to Elasticsearch for indexing
* **Bastion Host**Â â€”Â Builds & deploys the webapp; hosts the "`lattice-org`" app on port 9000; hosts the Rundeck server on port 4440
* **Rundeck Worker** â€”Â Worker node where all Rundeck ingestion jobs get executed

### Bootstrapping

Each instance-specific .tf file builds its own "`bootstrap.sh`" script with supporting files (dot files, scripts, packages) to be installed.

"`bootstrap.sh`" is composed of "`shared/prolog.sh`", followed by "`boot.tfpl`" and "`boot.sh`" files, followed by one or more  
"`install.tfpl`" and "`install.sh`" files (for each section), and ending with "`shared/epilog.sh`".  
"`bootstrap.sh`" could be composed of several sections, each generated by another .tf file (e.g. the bastion host installs both the  
webapp and Rundeck server).

These files are all "`aws_s3_object`" resources uploaded into the "`alprs-infra-dev`" bucket under "`userdata/`".

Since "`bootstrap.sh`" is often too big to be directly included as instance "userdata", each instance runs "`shared/s3boot.sh`" instead.  
That script then downloads the appropriate "`bootstrap.sh`" from S3 as "`/bootstrap.sh`" and executes it.

Log output from "`/bootstrap.sh`" is written to "`/bootstrap.log`".

Note that it can take over 10 minutes to fully bootstrap an instance, but if all succeeds,  
the final output in "`/bootstrap.log`" should be "`===== END BOOTSTRAP =====`".

### `check.sh` â€” Sanity Checks

You can run "`check.sh`" 10 minutes or so after bootstrapping new instances to perform quick sanity checks on each server:

```bash
$ ./check.sh
Retrieving Terraform output variables...DONE.

Checking host "postgresql"...
Bootstrapping completed.
Port 5432 listening.

Checking host "elasticsearch"...
Bootstrapping completed.
Port 9200 listening.
Port 9300 listening.
Port 5601 listening.
Port 9201 listening.
Port 9301 listening.
Port  443 listening.

Checking host "conductor"...
Bootstrapping completed.
Process "java" running.
Port 5701 listening.

Checking host "datastore"...
Bootstrapping completed.
Process "java" running.
Port 8080 listening.
Port 8443 listening.

Checking host "indexer"...
Bootstrapping completed.
Process "java" running.
Port 8080 listening.
Port 8443 listening.

Checking host "bastion"...
Bootstrapping completed.
Process "java" running.
Process "npm"  running.
Port 4440 listening.
Port 9000 listening.

Checking host "worker"...
Bootstrapping completed.
App "shuttle" available.
App "flapper" available.

ğŸ‘ Check successful.
```

## S3 Buckets

Seven S3 buckets are used by the stack, six directly provisioned by Terraform:

* **alprs-infra-dev**Â â€”Â Terraform state file and bootstrapping files for all instances _(manually created)_
* **alprs-config-dev**Â â€”Â YAML configuration files (with passwords) for Conductor/Datastore/Indexer/Shuttle
* **alprs-webapp-dev**Â â€”Â AstroMetrics frontend served via CloudFront
* **alprs-sftp-dev**Â â€”Â Incoming data from ALPR feeds via SFTP
* **alprs-media-dev** â€”Â Storage for ingested ALPR images
* **alprs-audit-dev**Â â€”Â Audit logs generated by Conductor/Datastore/Indexer; AWS logs for CloudFront/ELB/SFTP
* **alprs-backup-dev**Â â€”Â Copies of built service/webapp tarballs; PostgreSQL backups
