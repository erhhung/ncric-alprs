<joblist>
  <job>
    <context>
      <options preserveOrder='true'>
        <option name='region' secure='true' storagePath='keys/region' valueExposed='true'>
          <description>AWS region</description>
          <hidden>true</hidden>
        </option>
        <option name='api_url' secure='true' storagePath='keys/api_url' valueExposed='true'>
          <description>API endpoint</description>
          <hidden>true</hidden>
        </option>
        <option name='shuttle_args' secure='true' storagePath='keys/shuttle_args' valueExposed='true'>
          <description>Shuttle CLI args</description>
          <hidden>true</hidden>
        </option>
        <option name='db_host' secure='true' storagePath='keys/db_host' valueExposed='true'>
          <description>PostgreSQL host</description>
          <hidden>true</hidden>
        </option>
        <option name='db_name' secure='true' storagePath='keys/db_name' valueExposed='true'>
          <description>NCRIC database</description>
          <hidden>true</hidden>
        </option>
        <option name='db_user' secure='true' storagePath='keys/db_user' valueExposed='true'>
          <description>PostgreSQL user</description>
          <hidden>true</hidden>
        </option>
        <option name='db_pass' secure='true' storagePath='keys/db_pass' valueExposed='true'>
          <description>PostgreSQL password</description>
          <hidden>true</hidden>
        </option>
        <option name='client_id' secure='true' storagePath='keys/client_id' valueExposed='true'>
          <description>Auth0 client ID</description>
          <hidden>true</hidden>
        </option>
        <option name='ol_user' secure='true' storagePath='keys/ol_user' valueExposed='true'>
          <description>Auth0 user email</description>
          <hidden>true</hidden>
        </option>
        <option name='ol_pass' secure='true' storagePath='keys/ol_pass' valueExposed='true'>
          <description>Auth0 user password</description>
          <hidden>true</hidden>
        </option>
        <option name='fetch_size' value='10000' />
        <option name='chunk_size' value='20000' />
      </options>
    </context>
    <defaultTab>nodes</defaultTab>
    <description><![CDATA[Runs every 6 hours on 6-hour chunks of data
* Chooses the bottom of the hour that the job starts in, 24 hours ago, as the end date, and 6 hours before that as the start date:
  * e.g. A job started at 8 am 10/14 will cover 2-8 am 10/13.
* Flock data is pulled at midnight daily for the entire previous day]]></description>
    <dispatch>
      <excludePrecedence>true</excludePrecedence>
      <keepgoing>false</keepgoing>
      <rankOrder>ascending</rankOrder>
      <successOnEmptyNodeFilter>false</successOnEmptyNodeFilter>
      <threadcount>1</threadcount>
    </dispatch>
    <executionEnabled>true</executionEnabled>
    <id>8d4baaed-6f58-42b7-9513-42dddf4def44</id>
    <loglevel>INFO</loglevel>
    <multipleExecutions>true</multipleExecutions>
    <name>Recurring Flock: realtime - 6-hour</name>
    <nodeFilterEditable>false</nodeFilterEditable>
    <nodefilters>
      <filter>name: Worker</filter>
    </nodefilters>
    <nodesSelectedByDefault>true</nodesSelectedByDefault>
    <plugins />
    <schedule>
      <month month='*' />
      <time hour='2/6' minute='05' seconds='0' />
      <weekday day='*' />
      <year year='*' />
    </schedule>
    <scheduleEnabled>false</scheduleEnabled>
    <sequence keepgoing='false' strategy='node-first'>
      <command>
        <description>6 hours a day ago</description>
        <script><![CDATA[#!/usr/bin/env python3

from datetime import datetime, timedelta
from sqlalchemy import create_engine
from pyntegrationsncric.pyntegrations.ca_ncric.flock.integration_definitions import \
    FLOCKIntegration, FLOCKImageSourceIntegration, FLOCKAgencyStandardizationIntegration
import pyntegrationsncric.pyntegrations.ca_ncric.utils.openlattice_functions as of
import pandas as pd
import math
import os

agencies = [
  "Atherton PD",
  "CHP",
  "Danville PD",
  "GGB Vista Point",
  "Livermore PD",
  "Napa PD",
  "NCRIC",
  "Piedmont PD",
  "San Mateo PD",
  "Vacaville PD",
  "Vallejo PD"
]

# End datetime is the beginning of the hour that the job starts in
dt_end = datetime.now().replace(microsecond=0, second=0, minute=0)
dt_end = dt_end - timedelta(hours=24)
dt_start = dt_end - timedelta(hours=6)

db_host = "@option.db_host@"
db_name = "@option.db_name@"
db_user = "@option.db_user@"
db_pass = "@option.db_pass@"
engine = create_engine(f"postgresql://{db_user}:{db_pass}@{db_host}:5432/{db_name}")

api_url = "@option.api_url@"
fsize = @option.fetch_size@
limit = @option.chunk_size@

pyntegrations_path = os.environ.get("PYNTEGRATIONS_PATH")
pyntegrations_path += "/ca_ncric"
shuttle_path = os.environ.get("SHUTTLE_PATH")
shuttle_args = "@option.shuttle_args@"

try:
    for agency in agencies:
        agency_no_space = agency.replace(" ", "").replace("'", "")

        integration = FLOCKIntegration(sql=f'''
            select f."readid",f."timestamp", f."type", f."plate", f."confidence",
                f."latitude", f."longitude", f."cameraid", f."cameraname", f."platestate", f."speed",
                f."direction", f."model", f."hotlistid", f."hotlistname", f."cameralocationlat",
                f."cameralocationlon", f."cameranetworkid", f."cameranetworkname", s."standardized_agency_name"
            from flock_reads f
            left join standardized_agency_names_flock s
            on cast(f.cameranetworkid as text) = s."ol.id"
            where "timestamp" >= '{dt_start}'
              and "timestamp" <= '{dt_end}'
              and s.standardized_agency_name = '{agency}'
            ''',
            flight_path=pyntegrations_path+f"/ncric_flights/ncric_{agency_no_space}_flight.yaml",
            clean_table_name_suffix="recur",
            base_url=api_url)
        main_table = integration.clean_and_upload()

        print(f"Integrating MAIN table for {agency}!")
        result = engine.execute(f"select count(*) from {main_table}")
        records = pd.DataFrame(result).loc[0, 0]
        print(f"{records} records to process")
        chunks = math.ceil(records / limit)

        for i in range(chunks):
            print(f"Processing records {limit*i} through {limit*(i+1)}!")

            integration.integrate_table(
                sql=f"select * from {main_table} limit {limit} offset {limit*i}",
                drop_table_on_success=False,
                memory_size=4,
                shuttle_path=shuttle_path,
                shuttle_args=shuttle_args+f" --fetchsize {fsize} --upload-size {fsize}")

        of.drop_table(engine, main_table)

    print("Integrating IMAGE SOURCE table!")
    integration = FLOCKImageSourceIntegration(sql=f'''
        select distinct "cameraid", "cameraname" from flock_reads
        where "timestamp" >= '{dt_start}'
          and "timestamp" <= '{dt_end}'
        ''',
        base_url=api_url)
    image_source_table = integration.clean_and_upload()
    integration.integrate_table(
        clean_table_name=image_source_table,
        drop_table_on_success=True,
        shuttle_path=shuttle_path,
        shuttle_args=shuttle_args+f" --s3 PRODUCTION --fetchsize {fsize} --upload-size {fsize}")

    print("Integrating STANDARDIZED AGENCY table!")
    sagency_integration = FLOCKAgencyStandardizationIntegration(
        sql="select * from standardized_agency_names",
        base_url=api_url)
    sagency_integration.integrate_table(
        sql="select * from standardized_agency_names_flock",
        drop_table_on_success=True,
        shuttle_path=shuttle_path,
        shuttle_args=shuttle_args)

except Exception as e:
    raise RuntimeError(f"Something went wrong with the job! Error message: {str(e)}")]]></script>
        <scriptargs />
      </command>
    </sequence>
    <timeZone>America/Los_Angeles</timeZone>
    <uuid>8d4baaed-6f58-42b7-9513-42dddf4def44</uuid>
  </job>
</joblist>