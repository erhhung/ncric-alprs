<joblist>
  <job>
    <context>
      <options preserveOrder='true'>
        <option name='region' secure='true' storagePath='keys/region' valueExposed='true'>
          <description>AWS region</description>
          <hidden>true</hidden>
        </option>
        <option name='api_url' secure='true' storagePath='keys/api_url' valueExposed='true'>
          <description>API endpoint</description>
          <hidden>true</hidden>
        </option>
        <option name='shuttle_args' secure='true' storagePath='keys/shuttle_args' valueExposed='true'>
          <description>Shuttle CLI args</description>
          <hidden>true</hidden>
        </option>
        <option name='db_host' secure='true' storagePath='keys/db_host' valueExposed='true'>
          <description>PostgreSQL host</description>
          <hidden>true</hidden>
        </option>
        <option name='db_name' secure='true' storagePath='keys/db_name' valueExposed='true'>
          <description>NCRIC database</description>
          <hidden>true</hidden>
        </option>
        <option name='db_user' secure='true' storagePath='keys/db_user' valueExposed='true'>
          <description>PostgreSQL user</description>
          <hidden>true</hidden>
        </option>
        <option name='db_pass' secure='true' storagePath='keys/db_pass' valueExposed='true'>
          <description>PostgreSQL password</description>
          <hidden>true</hidden>
        </option>
        <option name='client_id' secure='true' storagePath='keys/client_id' valueExposed='true'>
          <description>Auth0 client ID</description>
          <hidden>true</hidden>
        </option>
        <option name='ol_user' secure='true' storagePath='keys/ol_user' valueExposed='true'>
          <description>Auth0 user email</description>
          <hidden>true</hidden>
        </option>
        <option name='ol_pass' secure='true' storagePath='keys/ol_pass' valueExposed='true'>
          <description>Auth0 user password</description>
          <hidden>true</hidden>
        </option>
        <option name='fetch_size' value='500' />
        <option name='upload_size' value='100' />
        <option name='chunk_size' value='1000' />
      </options>
    </context>
    <defaultTab>nodes</defaultTab>
    <description><![CDATA[Runs every 6 hours on 6-hour chunks of data
* Chooses the bottom of the hour that the job starts in, 24 hours ago, as the end date, and 6 hours before that as the start date:
  * e.g. A job started at 8 am 10/14 will cover 2-8 am 10/13.
* Flock data is pulled at midnight daily for the entire previous day]]></description>
    <dispatch>
      <excludePrecedence>true</excludePrecedence>
      <keepgoing>false</keepgoing>
      <rankOrder>ascending</rankOrder>
      <successOnEmptyNodeFilter>false</successOnEmptyNodeFilter>
      <threadcount>1</threadcount>
    </dispatch>
    <executionEnabled>true</executionEnabled>
    <id>8d4baaed-6f58-42b7-9513-42dddf4def44</id>
    <loglevel>INFO</loglevel>
    <multipleExecutions>true</multipleExecutions>
    <name>Recurring Flock: realtime - 6-hour</name>
    <nodeFilterEditable>false</nodeFilterEditable>
    <nodefilters>
      <filter>name: Worker</filter>
    </nodefilters>
    <nodesSelectedByDefault>true</nodesSelectedByDefault>
    <notification>
      <onfailure>
        <email attachLog='true' attachLogInFile='true' recipients='alprs.devops@maiveric.com' subject='[Rundeck:PROD] Job ${execution.status}: ${job.name}' />
      </onfailure>
    </notification>
    <notifyAvgDurationThreshold />
    <plugins />
    <schedule>
      <month month='*' />
      <time hour='2/6' minute='05' seconds='0' />
      <weekday day='*' />
      <year year='*' />
    </schedule>
    <scheduleEnabled>true</scheduleEnabled>
    <sequence keepgoing='false' strategy='node-first'>
      <command>
        <description>6 hours a day ago</description>
        <script><![CDATA[#!/usr/bin/env python3

# JOB: Recurring Flock: realtime - 6-hour

from datetime import datetime, timedelta
from sqlalchemy import create_engine
from pyntegrationsncric.pyntegrations.ca_ncric.flock.integration_definitions import \
    FLOCKIntegration, FLOCKImageSourceIntegration, FLOCKAgencyStandardizationIntegration
import pyntegrationsncric.pyntegrations.ca_ncric.utils.openlattice_functions as of
import pandas as pd
import math
import os

agencies = [
    "Anderson City PD",
    "Atherton PD",
    "Benicia PD",
    "Beverly Hills PD",
    "Campbell PD",
    "Colma PD",
    "Danville PD",
    "Dixon PD",
    "Fairfield PD",
    "Fremont PD",
    "GGB Vista Point",
    "Gilroy PD",
    "Hayward PD",
    "Hercules PD",
    "Hillsborough PD",
    "Kaiser Permanente Vallejo",
    "Livermore PD",
    "Millbrae PD",
    "Morgan Hill PD",
    "NCRIC",
    "Napa PD",
    "Novato PD",
    "Oakley PD",
    "Orinda PD",
    "Piedmont PD",
    "Pleasanton PD",
    "Salinas PD",
    "San Bruno PD",
    "San Joaquin County SO",
    "San Mateo County SO",
    "San Mateo PD",
    "San Ramon PD",
    "Santa Clara PD",
    "Vallejo PD",
]

# End datetime is the beginning of the hour that the job starts in
dt_end = datetime.now().replace(microsecond=0, second=0, minute=0)
dt_end = dt_end - timedelta(hours=24)
dt_start = dt_end - timedelta(hours=6)

# if start and end times are on separate days and the end time is after
# 00:00:00, split the range into 2 which ends and continues at midnight
if dt_start.day != dt_end.day and dt_end > dt_end.replace(hour=0):
    dt_12am = dt_end.replace(hour=0)
    dt_ranges = [(dt_start, dt_12am), (dt_12am, dt_end)]
else:
    dt_ranges = [(dt_start, dt_end)]

db_host = "@option.db_host@"
db_name = "@option.db_name@"
db_user = "@option.db_user@"
db_pass = "@option.db_pass@"
engine = create_engine(f"postgresql://{db_user}:{db_pass}@{db_host}:5432/{db_name}")

api_url = "@option.api_url@"
fsize = @option.fetch_size@
usize = @option.upload_size@
limit = @option.chunk_size@

pyntegrations_path = os.environ.get("PYNTEGRATIONS_PATH")
pyntegrations_path += "/ca_ncric"
shuttle_path = os.environ.get("SHUTTLE_PATH")
shuttle_args = "@option.shuttle_args@"

try:
    for dt_start, dt_end in dt_ranges:
        # raw Flock data gets pulled into separate day-of-week reads tables
        reads_table_name = "flock_reads_" + dt_start.strftime("%a").lower()
        print(f'\nIntegrating from raw reads table "{reads_table_name}"...')

        for agency in agencies:
            agency_no_space = agency.replace(" ", "")

            integration = FLOCKIntegration(sql=f'''
                SELECT f."readid",f."timestamp", f."type", f."plate", f."confidence",
                    f."latitude", f."longitude", f."cameraid", f."cameraname", f."platestate", f."speed",
                    f."direction", f."model", f."hotlistid", f."hotlistname", f."cameralocationlat",
                    f."cameralocationlon", f."cameranetworkid", f."cameranetworkname", s."standardized_agency_name"
                FROM {reads_table_name} f
                LEFT JOIN standardized_agency_names s
                ON f.cameranetworkname = s."ol.name"
                WHERE "timestamp" >= '{dt_start}'
                  AND "timestamp" <  '{dt_end}'
                  AND s.standardized_agency_name = '{agency}'
                ''',
                flight_path=pyntegrations_path+f"/ncric_flights/ncric_{agency_no_space}_flight.yaml",
                clean_table_name_suffix="recurring",
                base_url=api_url)
            main_table = integration.clean_and_upload()

            print(f"Integrating MAIN table for {agency}!")
            result = engine.execute(f"SELECT COUNT(*) FROM {main_table}")
            records = pd.DataFrame(result).iloc[0, 0]
            print(f"{records} records to process")
            chunks = math.ceil(records / limit)

            for i in range(chunks):
                print(f"Processing records {limit*i} through {limit*(i+1)}!")

                integration.integrate_table(
                    sql=f"SELECT * FROM {main_table} LIMIT {limit} OFFSET {limit*i}",
                    drop_table_on_success=False,
                    memory_size=4,
                    shuttle_path=shuttle_path,
                    shuttle_args=shuttle_args+f" --fetchsize {fsize} --upload-size {usize}")

            of.drop_table(engine, main_table)

        print("Integrating IMAGE SOURCE table!")
        integration = FLOCKImageSourceIntegration(sql=f'''
            SELECT DISTINCT "cameraid", "cameraname"
            FROM {reads_table_name}
            WHERE "timestamp" >= '{dt_start}'
              AND "timestamp" <  '{dt_end}'
            ''',
            base_url=api_url)
        image_source_table = integration.clean_and_upload()
        integration.integrate_table(
            clean_table_name=image_source_table,
            drop_table_on_success=True,
            shuttle_path=shuttle_path,
            shuttle_args=shuttle_args+f" --s3 PRODUCTION --fetchsize {fsize} --upload-size {usize}")

        print("Integrating STANDARDIZED AGENCY table!")
        sagency_integration = FLOCKAgencyStandardizationIntegration(
            sql="SELECT * FROM standardized_agency_names",
            base_url=api_url)
        sagency_integration.integrate_table(
            sql="SELECT * FROM standardized_agency_names",
            drop_table_on_success=False,
            shuttle_path=shuttle_path,
            shuttle_args=shuttle_args)

except Exception as e:
    raise RuntimeError(f"Something went wrong with the job! Error message: {str(e)}")]]></script>
        <scriptargs />
      </command>
    </sequence>
    <timeZone>America/Los_Angeles</timeZone>
    <uuid>8d4baaed-6f58-42b7-9513-42dddf4def44</uuid>
  </job>
</joblist>