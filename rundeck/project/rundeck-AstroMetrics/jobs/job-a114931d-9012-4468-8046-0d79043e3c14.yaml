- defaultTab: nodes
  description: |-
    Flock images - large data stream slows down regular Flock integration, so this is done separately
    Runs every 3 hours on 3-hour chunks of data:
    * Chooses the bottom of the hour that the job starts in, 24 hours ago, as the end date, and 3 hours before that as the start date:
      * e.g. A job started at 9 am 10/14 will cover 6-9 am 10/13.
    * Flock data is pulled at midnight daily for the entire previous day
  executionEnabled: true
  id: a114931d-9012-4468-8046-0d79043e3c14
  loglevel: INFO
  multipleExecutions: true
  name: 'Recurring Flock - Images: realtime - 3-hour'
  nodeFilterEditable: false
  nodefilters:
    dispatch:
      excludePrecedence: true
      keepgoing: false
      rankOrder: ascending
      successOnEmptyNodeFilter: false
      threadcount: '1'
    filter: 'name: Worker'
  nodesSelectedByDefault: true
  notification:
    onfailure:
      email:
        attachLog: true
        attachLogInFile: true
        recipients: alprs.devops@maiveric.com
        subject: '[Rundeck:PROD] Job ${execution.status}: ${job.name}'
  notifyAvgDurationThreshold: null
  options:
  - description: AWS region
    hidden: true
    name: region
    secure: true
    storagePath: keys/region
    valueExposed: true
  - description: API endpoint
    hidden: true
    name: api_url
    secure: true
    storagePath: keys/api_url
    valueExposed: true
  - description: Shuttle CLI args
    hidden: true
    name: shuttle_args
    secure: true
    storagePath: keys/shuttle_args
    valueExposed: true
  - description: PostgreSQL host
    hidden: true
    name: db_host
    secure: true
    storagePath: keys/db_host
    valueExposed: true
  - description: NCRIC database
    hidden: true
    name: db_name
    secure: true
    storagePath: keys/db_name
    valueExposed: true
  - description: PostgreSQL user
    hidden: true
    name: db_user
    secure: true
    storagePath: keys/db_user
    valueExposed: true
  - description: PostgreSQL password
    hidden: true
    name: db_pass
    secure: true
    storagePath: keys/db_pass
    valueExposed: true
  - description: Auth0 client ID
    hidden: true
    name: client_id
    secure: true
    storagePath: keys/client_id
    valueExposed: true
  - description: Auth0 user email
    hidden: true
    name: ol_user
    secure: true
    storagePath: keys/ol_user
    valueExposed: true
  - description: Auth0 user password
    hidden: true
    name: ol_pass
    secure: true
    storagePath: keys/ol_pass
    valueExposed: true
  - name: fetch_size
    value: '500'
  - name: upload_size
    value: '100'
  - name: chunk_size
    value: '500'
  plugins:
    ExecutionLifecycle: null
  schedule:
    month: '*'
    time:
      hour: 0/3
      minute: '20'
      seconds: '0'
    weekday:
      day: '*'
    year: '*'
  scheduleEnabled: true
  sequence:
    commands:
    - description: 3 hours a day ago
      script: |-
        #!/usr/bin/env python3

        # JOB: Recurring Flock - Images: realtime - 3-hour

        from datetime import datetime, timedelta
        from sqlalchemy import create_engine
        from pyntegrationsncric.pyntegrations.ca_ncric.flock.integration_definitions \
            import FLOCKImagesIntegration
        import pyntegrationsncric.pyntegrations.ca_ncric.utils.openlattice_functions as of
        import pandas as pd
        import math
        import os

        agencies = [
            "Anderson City PD",
            "Atherton PD",
            "Benicia PD",
            "Beverly Hills PD",
            "Campbell PD",
            "Colma PD",
            "Danville PD",
            "Dixon PD",
            "Fairfield PD",
            "Fremont PD",
            "GGB Vista Point",
            "Gilroy PD",
            "Hayward PD",
            "Hercules PD",
            "Hillsborough PD",
            "Kaiser Permanente Vallejo",
            "Livermore PD",
            "Millbrae PD",
            "Morgan Hill PD",
            "NCRIC",
            "Napa PD",
            "Novato PD",
            "Oakley PD",
            "Orinda PD",
            "Piedmont PD",
            "Pleasanton PD",
            "Salinas PD",
            "San Bruno PD",
            "San Joaquin County SO",
            "San Mateo County SO",
            "San Mateo PD",
            "San Ramon PD",
            "Santa Clara PD",
            "Vallejo PD",
        ]

        # End datetime is the start of the hour that the job starts in.
        # IMPORTANT! dt_end needs to be in UTC because it's used in the
        # SQL to filter against the "timestamp" column, which is in UTC,
        # and it's also used to select the day-of-week table to look in.
        dt_end = datetime.utcnow().replace(microsecond=0, second=0, minute=0)
        dt_end = dt_end - timedelta(hours=24)
        dt_start = dt_end - timedelta(hours=3)

        # if start and end times are on separate days and the end time is after
        # 00:00:00, split the range into 2 which ends and continues at midnight
        if dt_start.day != dt_end.day and dt_end > dt_end.replace(hour=0):
            dt_12am = dt_end.replace(hour=0)
            dt_ranges = [(dt_start, dt_12am), (dt_12am, dt_end)]
        else:
            dt_ranges = [(dt_start, dt_end)]

        db_host = "@option.db_host@"
        db_name = "@option.db_name@"
        db_user = "@option.db_user@"
        db_pass = "@option.db_pass@"
        engine = create_engine(f"postgresql://{db_user}:{db_pass}@{db_host}:5432/{db_name}")

        client_id = "@option.client_id@"
        ol_user = "@option.ol_user@"
        ol_pass = "@option.ol_pass@"
        jwt = of.get_jwt(client_id=client_id, username=ol_user, password=ol_pass)

        api_url = "@option.api_url@"
        fsize = @option.fetch_size@
        usize = @option.upload_size@
        limit = @option.chunk_size@

        pyntegrations_path = os.environ.get("PYNTEGRATIONS_PATH")
        pyntegrations_path += "/ca_ncric"
        shuttle_path = os.environ.get("SHUTTLE_PATH")
        shuttle_args = "@option.shuttle_args@"
        shuttle_args += f" --s3 PRODUCTION --fetchsize {fsize} --upload-size {usize}"

        try:
            for dt_start, dt_end in dt_ranges:
                # raw Flock data gets pulled into separate day-of-week reads tables
                reads_table_name = "flock_reads_" + dt_start.strftime("%a").lower()
                print(f'\nIntegrating from raw reads table "{reads_table_name}"...')

                for agency in agencies:
                    agency_no_space = agency.replace(" ", "")

                    print(f"\nIntegrating IMAGES table for {agency}!")
                    result = engine.execute(f'''
                        SELECT COUNT(*) FROM (
                            SELECT f.readid, f.image, s.standardized_agency_name
                            FROM {reads_table_name} f
                            LEFT JOIN standardized_agency_names s ON f.cameranetworkname = s."ol.name"
                            WHERE "timestamp" >= '{dt_start}'
                              AND "timestamp" <  '{dt_end}'
                              AND s.standardized_agency_name = '{agency}'
                        ) t''')
                    records = pd.DataFrame(result).iloc[0, 0]
                    print(f"{records} records to process")
                    chunks = math.ceil(records / limit)

                    for i in range(chunks):
                        print(f"Processing records {limit*i} through {limit*(i+1)}!")

                        query = f'''
                            SELECT f.readid::text || '_FLOCK' AS "vehicle_record_id",
                                f.image AS "LPRVehiclePlatePhoto",
                                s.standardized_agency_name
                            FROM {reads_table_name} f
                            LEFT JOIN standardized_agency_names s ON f.cameranetworkname = s."ol.name"
                            WHERE "timestamp" >= '{dt_start}'
                              AND "timestamp" <  '{dt_end}'
                              AND s.standardized_agency_name = '{agency}'
                            ORDER BY f.readid
                            LIMIT {limit} OFFSET {limit*i}
                            '''
                        jwt = of.refresh_jwt_if_needed(jwt=jwt, min_ttl_mins=120,
                            client_id=client_id, username=ol_user, password=ol_pass)
                        images_integration = FLOCKImagesIntegration(jwt=jwt, sql=query, base_url=api_url)
                        images_integration.integrate_table(sql=query, drop_table_on_success=True,
                            flight_path=pyntegrations_path+f"/ncric_image_flights/ncric_{agency_no_space}_images_flight.yaml",
                            memory_size=9,
                            shuttle_path=shuttle_path,
                            shuttle_args=shuttle_args)

        except Exception as e:
            raise RuntimeError(f"Something went wrong with the job! Error message: {str(e)}")
    keepgoing: false
    strategy: node-first
  timeZone: America/Los_Angeles
  uuid: a114931d-9012-4468-8046-0d79043e3c14
