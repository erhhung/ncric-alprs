- defaultTab: nodes
  description: |-
    Gets NCRIC hotlist from SFTP every day
    * Appends to _all table and overwrites _daily table in the NCRIC org
    * Wipes yesterday's hotlist entity sets
  executionEnabled: true
  id: 933ce02e-d364-4f6e-8db2-dea0ef56c966
  loglevel: INFO
  name: 'Recurring Integration & Staging: Hotlist - daily'
  nodeFilterEditable: false
  nodefilters:
    dispatch:
      excludePrecedence: true
      keepgoing: false
      rankOrder: ascending
      successOnEmptyNodeFilter: false
      threadcount: '1'
    filter: 'name: Worker'
  nodesSelectedByDefault: true
  options:
  - description: AWS region
    hidden: true
    name: region
    secure: true
    storagePath: keys/region
    valueExposed: true
  - description: SFTP bucket
    hidden: true
    name: s3_bucket
    secure: true
    storagePath: keys/s3_bucket
    valueExposed: true
  - description: Bucket prefix
    hidden: true
    name: s3_prefix
    secure: true
    storagePath: keys/s3_prefix/hotlist
    valueExposed: true
  - description: API endpoint
    hidden: true
    name: api_url
    secure: true
    storagePath: keys/api_url
    valueExposed: true
  - description: PostgreSQL host
    hidden: true
    name: db_host
    secure: true
    storagePath: keys/db_host
    valueExposed: true
  - description: NCRIC database
    hidden: true
    name: db_name
    secure: true
    storagePath: keys/db_name
    valueExposed: true
  - description: PostgreSQL user
    hidden: true
    name: db_user
    secure: true
    storagePath: keys/db_user
    valueExposed: true
  - description: PostgreSQL password
    hidden: true
    name: db_pass
    secure: true
    storagePath: keys/db_pass
    valueExposed: true
  plugins:
    ExecutionLifecycle: null
  schedule:
    month: '*'
    time:
      hour: '06'
      minute: '00'
      seconds: '0'
    weekday:
      day: '*'
    year: '*'
  scheduleEnabled: false
  sequence:
    commands:
    - description: Get hotlist from SFTP
      script: |-
        #!/usr/bin/env python3

        from sqlalchemy import create_engine
        from datetime import datetime
        from io import StringIO
        import pandas as pd
        import boto3

        region = "@option.region@"
        s3_bucket = "@option.s3_bucket@"
        s3_prefix = "@option.s3_prefix@"

        session = boto3.session.Session(region_name=region)
        s3 = session.resource("s3")
        bucket = s3.Bucket(name=s3_bucket)
        files = bucket.objects.filter(Prefix=s3_prefix)

        db_host = "@option.db_host@"
        db_name = "@option.db_name@"
        db_user = "@option.db_user@"
        db_pass = "@option.db_pass@"
        engine = create_engine(f"postgresql://{db_user}:{db_pass}@{db_host}:5432/{db_name}")
        data_table = "hotlist_daily"

        idx = 0
        for obj in files:
            idx += 1
            text = obj.get()["Body"].read().decode("utf-8")

            # get the date and time info from the first row
            table_info = text.partition("\n")[0]

            # get the body of the hotlist
            text_string = StringIO(text)
            df = pd.read_fwf(text_string, widths=[7, 12], names=["plate", "agencysubmitted"],
                             skiprows=1, skipfooter=1)
            print("hotlist data size (minus header and footer):", df.shape)

            # check last row (also non-uniform)
            # print("last row:", df.tail(1))

            # make it a new column in the dataframe (for rolling table in atlas)
            c = table_info.replace("DATE", "")
            print("cleaned table_info:", c)
            date_time = datetime.strptime(c, "%m/%d/%Y %H:%M")
            print("date_time of hotlist:", date_time)
            df["submitted"] = date_time

            ###### UPLOAD TO DATABASE
            df.to_sql(data_table, engine, chunksize=1000, if_exists="replace")
            print(f"Uploaded {len(df)} rows to {data_table}")

        print("number of files:", idx)
    - description: Sleep for 5 minutes
      exec: sleep 300
    - description: 'Wipe the 2 hotlist entity sets every night - WON''T WORK YET: NEED NEW ENTITY SETS'
      script: |-
        #!/usr/bin/env python3

        import openlattice
        import olpy

        api_url = "@option.api_url@"
        config = olpy.get_config(base_url=api_url)
        dataAPI = openlattice.DataApi(openlattice.ApiClient(config))
        entitySetsApi = openlattice.EntitySetsApi(openlattice.ApiClient(config))

        ###### VEHICLES
        entity_set = "astrometrics_1446ff84711242ec828df181f45e4d20_apphotlistvehicles"
        entity_set_id = entitySetsApi.get_entity_set_id(entity_set)
        vehicles = dataAPI.load_entity_set_data(entity_set_id)
        vehicle_id_list = [ent["openlattice.@id"][0] for ent in vehicles]
        print(f"There are {len(vehicle_id_list)} entities in the Hotlist vehicle entity set.")

        dataAPI.delete_entities(entity_set_id=entity_set_id, type="Hard",
                                request_body=vehicle_id_list)
        vehicles = dataAPI.load_entity_set_data(entity_set_id)
        print(f"Deleted! There are now {len(vehicles)} entities in the Hotlist vehicle entity set.")

        ###### READS
        entity_set = "astrometrics_1446ff84711242ec828df181f45e4d20_apphotlistreads"
        entity_set_id = entitySetsApi.get_entity_set_id(entity_set)
        reads = dataAPI.load_entity_set_data(entity_set_id)
        reads_id_list = [ent["openlattice.@id"][0] for ent in reads]
        print(f"There are {len(reads_id_list)} entities in the Hotlist reads entity set.")

        dataAPI.delete_entities(entity_set_id=entity_set_id, type="Hard",
                                request_body=reads_id_list)
        reads = dataAPI.load_entity_set_data(entity_set_id)
        print(f"Deleted! There are now {len(reads)} entities in the Hotlist reads entity set.")
    keepgoing: true
    strategy: node-first
  timeZone: America/Los_Angeles
  uuid: 933ce02e-d364-4f6e-8db2-dea0ef56c966
