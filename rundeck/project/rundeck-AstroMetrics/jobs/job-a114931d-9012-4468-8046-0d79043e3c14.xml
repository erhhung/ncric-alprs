<joblist>
  <job>
    <context>
      <options preserveOrder='true'>
        <option name='region' secure='true' storagePath='keys/region' valueExposed='true'>
          <description>AWS region</description>
          <hidden>true</hidden>
        </option>
        <option name='api_url' secure='true' storagePath='keys/api_url' valueExposed='true'>
          <description>API endpoint</description>
          <hidden>true</hidden>
        </option>
        <option name='shuttle_args' secure='true' storagePath='keys/shuttle_args' valueExposed='true'>
          <description>Shuttle CLI args</description>
          <hidden>true</hidden>
        </option>
        <option name='db_host' secure='true' storagePath='keys/db_host' valueExposed='true'>
          <description>PostgreSQL host</description>
          <hidden>true</hidden>
        </option>
        <option name='db_name' secure='true' storagePath='keys/db_name' valueExposed='true'>
          <description>NCRIC database</description>
          <hidden>true</hidden>
        </option>
        <option name='db_user' secure='true' storagePath='keys/db_user' valueExposed='true'>
          <description>PostgreSQL user</description>
          <hidden>true</hidden>
        </option>
        <option name='db_pass' secure='true' storagePath='keys/db_pass' valueExposed='true'>
          <description>PostgreSQL password</description>
          <hidden>true</hidden>
        </option>
        <option name='client_id' secure='true' storagePath='keys/client_id' valueExposed='true'>
          <description>Auth0 client ID</description>
          <hidden>true</hidden>
        </option>
        <option name='ol_user' secure='true' storagePath='keys/ol_user' valueExposed='true'>
          <description>Auth0 user email</description>
          <hidden>true</hidden>
        </option>
        <option name='ol_pass' secure='true' storagePath='keys/ol_pass' valueExposed='true'>
          <description>Auth0 user password</description>
          <hidden>true</hidden>
        </option>
        <option name='fetch_size' value='5000' />
        <option name='chunk_size' value='40000' />
      </options>
    </context>
    <defaultTab>nodes</defaultTab>
    <description><![CDATA[Flock images - large data stream slows down regular Flock integration, so this is done separately
Runs every 3 hours on 3-hour chunks of data:
* Chooses the bottom of the hour that the job starts in, 24 hours ago, as the end date, and 3 hours before that as the start date:
  * e.g. A job started at 9 am 10/14 will cover 6-9 am 10/13.
* Flock data is pulled at midnight daily for the entire previous day]]></description>
    <dispatch>
      <excludePrecedence>true</excludePrecedence>
      <keepgoing>false</keepgoing>
      <rankOrder>ascending</rankOrder>
      <successOnEmptyNodeFilter>false</successOnEmptyNodeFilter>
      <threadcount>1</threadcount>
    </dispatch>
    <executionEnabled>true</executionEnabled>
    <id>a114931d-9012-4468-8046-0d79043e3c14</id>
    <loglevel>INFO</loglevel>
    <multipleExecutions>true</multipleExecutions>
    <name>Recurring Flock - Images: realtime - 3-hour</name>
    <nodeFilterEditable>false</nodeFilterEditable>
    <nodefilters>
      <filter>name: Worker</filter>
    </nodefilters>
    <nodesSelectedByDefault>true</nodesSelectedByDefault>
    <plugins />
    <schedule>
      <month month='*' />
      <time hour='0/3' minute='20' seconds='0' />
      <weekday day='*' />
      <year year='*' />
    </schedule>
    <scheduleEnabled>false</scheduleEnabled>
    <sequence keepgoing='false' strategy='node-first'>
      <command>
        <description>3 hours a day ago</description>
        <script><![CDATA[#!/usr/bin/env python3

from datetime import datetime, timedelta
from sqlalchemy import create_engine
from pyntegrationsncric.pyntegrations.ca_ncric.flock.integration_definitions \
  import FLOCKImagesIntegration
import pandas as pd
import math
import os

agencies = [
  "Atherton PD",
  "CHP",
  "Danville PD",
  "GGB Vista Point",
  "Livermore PD",
  "Napa PD",
  "NCRIC",
  "Piedmont PD",
  "San Mateo PD",
  "Vacaville PD",
  "Vallejo PD"
]

# End datetime is the beginning of the hour that the job starts in
dt_end = datetime.now().replace(microsecond=0, second=0, minute=0)
dt_end = dt_end - timedelta(hours=24)
dt_start = dt_end - timedelta(hours=3)

db_host = "@option.db_host@"
db_name = "@option.db_name@"
db_user = "@option.db_user@"
db_pass = "@option.db_pass@"
engine = create_engine(f"postgresql://{db_user}:{db_pass}@{db_host}:5432/{db_name}")

api_url = "@option.api_url@"
fsize = @option.fetch_size@
limit = @option.chunk_size@

pyntegrations_path = os.environ.get("PYNTEGRATIONS_PATH")
pyntegrations_path += "/ca_ncric"
shuttle_path = os.environ.get("SHUTTLE_PATH")
shuttle_args = "@option.shuttle_args@"
shuttle_args += f" --s3 PRODUCTION --fetchsize {fsize} --upload-size {fsize}"

try:
    for agency in agencies:
        agency_no_space = agency.replace(" ", "").replace("'", "")

        print(f"Integrating IMAGES table for {agency}!")
        result = engine.execute(f'''
            select count(*) from (
                select f.readid, f.image, s.standardized_agency_name
                from flock_reads f
                left join standardized_agency_names_flock s
                on cast(f.cameranetworkid as text) = s."ol.id"
                where "timestamp" >= '{dt_start}'
                  and "timestamp" <= '{dt_end}'
                  and s.standardized_agency_name = '{agency}'
            ) t''')
        records = pd.DataFrame(result).loc[0, 0]
        print(f"{records} records to process")
        chunks = math.ceil(records / limit)

        for i in range(chunks):
            print(f"Processing records {limit*i} through {limit*(i+1)}!")

            query = f'''
                select f.readid::text || '_FLOCK' as "vehicle_record_id",
                    f.image as "LPRVehiclePlatePhoto",
                    s.standardized_agency_name
                from flock_reads f
                left join standardized_agency_names_flock s
                on cast(f.cameranetworkid as text) = s."ol.id"
                where "timestamp" >= '{dt_start}'
                  and "timestamp" <= '{dt_end}'
                  and s.standardized_agency_name = '{agency}'
                order by f.readid
                limit {limit} offset {limit*i}
                '''
            images_integration = FLOCKImagesIntegration(sql=query, base_url=api_url)
            images_integration.integrate_table(sql=query, drop_table_on_success=True,
                flight_path=pyntegrations_path+f"/ncric_image_flights/ncric_{agency_no_space}_images_flight.yaml",
                memory_size=9,
                shuttle_path=shuttle_path,
                shuttle_args=shuttle_args)

except Exception as e:
    raise RuntimeError(f"Something went wrong with the job! Error message: {str(e)}")]]></script>
        <scriptargs />
      </command>
    </sequence>
    <timeZone>America/Los_Angeles</timeZone>
    <uuid>a114931d-9012-4468-8046-0d79043e3c14</uuid>
  </job>
</joblist>