<joblist>
  <job>
    <context>
      <options preserveOrder='true'>
        <option name='region' secure='true' storagePath='keys/region' valueExposed='true'>
          <description>AWS region</description>
          <hidden>true</hidden>
        </option>
        <option name='s3_bucket' secure='true' storagePath='keys/s3_bucket' valueExposed='true'>
          <description>SFTP bucket</description>
          <hidden>true</hidden>
        </option>
        <option name='s3_prefix' secure='true' storagePath='keys/s3_prefix/boss4' valueExposed='true'>
          <description>Bucket prefix</description>
          <hidden>true</hidden>
        </option>
        <option name='api_url' secure='true' storagePath='keys/api_url' valueExposed='true'>
          <description>API endpoint</description>
          <hidden>true</hidden>
        </option>
        <option name='shuttle_args' secure='true' storagePath='keys/shuttle_args' valueExposed='true'>
          <description>Shuttle CLI args</description>
          <hidden>true</hidden>
        </option>
        <option name='db_host' secure='true' storagePath='keys/db_host' valueExposed='true'>
          <description>PostgreSQL host</description>
          <hidden>true</hidden>
        </option>
        <option name='db_name' secure='true' storagePath='keys/db_name' valueExposed='true'>
          <description>NCRIC database</description>
          <hidden>true</hidden>
        </option>
        <option name='db_user' secure='true' storagePath='keys/db_user' valueExposed='true'>
          <description>PostgreSQL user</description>
          <hidden>true</hidden>
        </option>
        <option name='db_pass' secure='true' storagePath='keys/db_pass' valueExposed='true'>
          <description>PostgreSQL password</description>
          <hidden>true</hidden>
        </option>
        <option name='client_id' secure='true' storagePath='keys/client_id' valueExposed='true'>
          <description>Auth0 client ID</description>
          <hidden>true</hidden>
        </option>
        <option name='ol_user' secure='true' storagePath='keys/ol_user' valueExposed='true'>
          <description>Auth0 user email</description>
          <hidden>true</hidden>
        </option>
        <option name='ol_pass' secure='true' storagePath='keys/ol_pass' valueExposed='true'>
          <description>Auth0 user password</description>
          <hidden>true</hidden>
        </option>
        <option name='fetch_size' value='5000' />
      </options>
    </context>
    <defaultTab>nodes</defaultTab>
    <description><![CDATA[Catchup job for Fremont only, from BOSS4 into Aurora and the agency entity sets
* End datetime range is the bottom of the hour that the job is run. For a job at 9:10 am, covers 8-9 am.
* Does the whole day for the day before yesterday
* All BOSS4 data is in UTC time]]></description>
    <dispatch>
      <excludePrecedence>true</excludePrecedence>
      <keepgoing>false</keepgoing>
      <rankOrder>ascending</rankOrder>
      <successOnEmptyNodeFilter>false</successOnEmptyNodeFilter>
      <threadcount>1</threadcount>
    </dispatch>
    <executionEnabled>true</executionEnabled>
    <id>fa1e1542-e401-48d9-b4e0-523624558eb5</id>
    <loglevel>INFO</loglevel>
    <multipleExecutions>true</multipleExecutions>
    <name>Recurring BOSS4 - Fremont: asynchronous - goes back two days</name>
    <nodeFilterEditable>false</nodeFilterEditable>
    <nodefilters>
      <filter>name: Worker</filter>
    </nodefilters>
    <nodesSelectedByDefault>true</nodesSelectedByDefault>
    <notification>
      <onfailure>
        <email attachLog='true' attachLogInFile='true' recipients='alprs.devops@maiveric.com' subject='[Rundeck:PROD] Job ${execution.status}: ${job.name}' />
      </onfailure>
    </notification>
    <notifyAvgDurationThreshold />
    <plugins />
    <schedule>
      <month month='*' />
      <time hour='04' minute='00' seconds='0' />
      <weekday day='*' />
      <year year='*' />
    </schedule>
    <scheduleEnabled>true</scheduleEnabled>
    <sequence keepgoing='false' strategy='node-first'>
      <command>
        <description>2 days ago</description>
        <script><![CDATA[#!/usr/bin/env python3

# JOB: Recurring BOSS4 - Fremont: asynchronous - goes back two days

from datetime import datetime, timedelta
from sqlalchemy import create_engine
from pyntegrationsncric.pyntegrations.ca_ncric.boss4.integration_definitions \
  import BOSS4Integration, BOSS4ImagesIntegration, BOSS4AgenciesIntegration, \
  BOSS4ImageSourcesIntegration, BOSS4AgenciesStandardizedIntegration
import pyntegrationsncric.pyntegrations.ca_ncric.utils.openlattice_functions as of
import math
import os

s3_bucket = "@option.s3_bucket@"
s3_prefix = "@option.s3_prefix@"

# Include start date and end date - IN UTC
dt_end = datetime.now().replace(microsecond=0, second=0, minute=0, hour=0)
dt_start = dt_end - timedelta(days=2)
dt_end = dt_end - timedelta(days=1)
print("dt_start:", dt_start)
print("dt_end:", dt_end)

# interval to search over, so if 2021-3-8 4:00:00 to 2021-3-8 14:00:00
# hour interval is 2, then you would search 2021-3-8 4:00:00 - 6:00:00
# and so on. The integration would run 5 times in total.
hour_interval = 1
delta = dt_end - dt_start
# find the total number of hours between start date and end date
total_hours, remainder = divmod(delta.seconds, 3600)
total_hours += delta.days * 24
print("total_hours:", total_hours)

# get total number of 1-hour intervals (or x number of intervals)
# between the start time and end time rounded up so that you will
# always have more search over this than taking the min b/w that
# and the end date so that it always caps at the dt_end
total_intervals = math.ceil(total_hours / hour_interval)
print("total_intervals:", total_intervals)

agency = "Fremont PD"
agency_no_space = agency.replace(" ", "")

api_url = "@option.api_url@"
fsize = @option.fetch_size@

pyntegrations_path = os.environ.get("PYNTEGRATIONS_PATH")
pyntegrations_path += "/ca_ncric"
shuttle_path = os.environ.get("SHUTTLE_PATH")
shuttle_args = "@option.shuttle_args@"

db_host = "@option.db_host@"
db_name = "@option.db_name@"
db_user = "@option.db_user@"
db_pass = "@option.db_pass@"
engine = create_engine(f"postgresql://{db_user}:{db_pass}@{db_host}:5432/{db_name}")

for i in range(total_intervals):
    sdate = dt_start + timedelta(hours=i*hour_interval)
    edate = min(dt_start + timedelta(hours=(i+1)*hour_interval), dt_end)
    print(f"Searching dates from {sdate} to {edate} to integrate MAIN BOSS4 table!")

    main_integration = BOSS4Integration(
        date_start=str(sdate),
        date_end=str(edate),
        raw_table_name="boss4_catchup_fremont",
        raw_table_name_images="boss4_catchup_images_fremont",
        s3_bucket=s3_bucket,
        s3_prefix=s3_prefix)

    main_table_name, images_table_name = main_integration.get_raw_data_from_s3()
    print(main_table_name, images_table_name)
    print("Got the raw data!")

    images_integration = BOSS4ImagesIntegration(
        sql=f"select * from {images_table_name}",
        clean_table_name_root="boss4_catchup_images_fremont",
        base_url=api_url)

    images_clean_table_name = images_integration.clean_and_upload()
    print("Got the images data!")

    try:
        print(f"Integrating MAIN table for {agency}!")
        # integrate all with new tables
        main_integration.integrate_table(
            sql=f"select * from {main_table_name} where standardized_agency_name = '{agency}'",
            flight_path=pyntegrations_path+f"/ncric_flights/ncric_{agency_no_space}_flight.yaml",
            memory_size=3,
            shuttle_path=shuttle_path,
            shuttle_args=shuttle_args+f" --fetchsize {fsize} --upload-size {fsize}")
        print("Passed: integrate_table MAIN table")

        print(f"Integrating IMAGES table for {agency}!")
        # integrate images table
        images_integration.integrate_table(
            sql=f"select * from {images_clean_table_name} where standardized_agency_name = '{agency}'",
            flight_path=pyntegrations_path+f"/ncric_image_flights/ncric_{agency_no_space}_images_flight.yaml",
            memory_size=5,
            shuttle_path=shuttle_path,
            shuttle_args=shuttle_args+f" --s3 PRODUCTION --fetchsize {fsize} --upload-size {fsize}")
        print("Passed: integrate_table IMAGES table")

        print("Integrating AGENCIES table!")
        # integrate agencies associate only
        integration = BOSS4AgenciesIntegration(
            sql=f'select distinct "agency_id", "agencyName", "datasource", "agencyAcronym" from {main_table_name}',
            clean_table_name_root="boss4_agencies_catchup_fremont",
            base_url=api_url)
        agencies_table_name = integration.clean_and_upload()
        integration.integrate_table(
            clean_table_name=agencies_table_name,
            shuttle_path=shuttle_path,
            shuttle_args=shuttle_args)

        print("Integrating IMAGE SOURCES table!")
        # integrate image sources associate only
        integration = BOSS4ImageSourcesIntegration(
            sql=f'select distinct "camera_id", "LPRCameraName", "datasource" from {main_table_name}',
            clean_table_name_root="boss4_imagesources_catchup_fremont",
            base_url=api_url)
        image_sources_table = integration.clean_and_upload()
        integration.integrate_table(
            clean_table_name=image_sources_table,
            shuttle_path=shuttle_path,
            shuttle_args=shuttle_args)

        print("Integrating STANDARDIZED AGENCIES table!")
        sagency_integration = BOSS4AgenciesStandardizedIntegration(
            sql="""select distinct standardized_agency_name from standardized_agency_names
                where \"ol.datasource\" = 'BOSS4'
                """,
            base_url=api_url)
        sagency_integration.integrate_table(
            sql="""select distinct standardized_agency_name from standardized_agency_names
                where \"ol.datasource\" = 'BOSS4'
                """,
            shuttle_path=shuttle_path,
            shuttle_args=shuttle_args)

    except Exception as e:
        main_integration.drop_main_table()
        main_integration.drop_images_table()
        raise RuntimeError(f"Something went wrong with the job! Dropping tables. Error message: {str(e)}")

    main_integration.drop_main_table()
    main_integration.drop_images_table()

    # Drop other tables after integration is complete
    of.drop_table(engine, agencies_table_name)
    of.drop_table(engine, image_sources_table)]]></script>
        <scriptargs />
      </command>
    </sequence>
    <timeZone>America/Los_Angeles</timeZone>
    <uuid>fa1e1542-e401-48d9-b4e0-523624558eb5</uuid>
  </job>
</joblist>